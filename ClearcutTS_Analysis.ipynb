{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c134e9cd",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center>Trends In Forest Recovery After Stand Replacing Disturbance: A Spectrotemporal Evaluation Of Productivity In Southeastern Pine Forests</center></h1>\n",
    "\n",
    "<h4><center> Daniel J. Putnam </center></h4>\n",
    "\n",
    "<center> For partial fulfillment of the reqiurements for the Master of Science degree </center>\n",
    "<center> College of Natural Resources and Environment </center>\n",
    "<center> Virginia Polytechnic Institute and State University </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73021364",
   "metadata": {},
   "source": [
    "## Analysis Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc87dce3",
   "metadata": {},
   "source": [
    "### _Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7a09c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geemap\n",
    "import ee\n",
    "import folium\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from datetime import datetime\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce85ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ee.Authenticate(auth_mode='paste')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1cdbf3",
   "metadata": {},
   "source": [
    "### _Imports_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25606093",
   "metadata": {},
   "outputs": [],
   "source": [
    "LS5 = ee.ImageCollection(\"LANDSAT/LT05/C01/T1_SR\") # landsat 5\n",
    "LS7 = ee.ImageCollection(\"LANDSAT/LE07/C01/T1_SR\") # landsat 7\n",
    "LS8 = ee.ImageCollection(\"LANDSAT/LC08/C01/T1_SR\") # landsat 8\n",
    "LCMS = ee.ImageCollection(\"USFS/GTAC/LCMS/v2021-7\") # landscape Change Monitoring System\n",
    "NLCD_col = ee.ImageCollection(\"USGS/NLCD_RELEASES/2019_REL/NLCD\") # national landcover Database\n",
    "loblolly = ee.FeatureCollection(\"users/dputnam21/us_eco_l3_NEW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e295bb3",
   "metadata": {},
   "source": [
    "### _Priliminary set-up_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcc39449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sample date range for disturbances\n",
    "startingD = ee.Date.fromYMD(1994,1,1)\n",
    "endingD = ee.Date.fromYMD(2011,12,31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed0f26",
   "metadata": {},
   "source": [
    "### _Landsat Preprocessing_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6658501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud masking based on the QA band : code taken from landsat example in data catalog in EE\n",
    "def LScloudMask(image):\n",
    "  qa = image.select('pixel_qa')\n",
    "    # removing cloud pixels if confiance is high, cloud shadow, snow\n",
    "  cloud = qa.bitwiseAnd(1 << 5).And(qa.bitwiseAnd(1 << 7)) \\\n",
    "            .Or(qa.bitwiseAnd(1 << 3)) \\\n",
    "            .Or(qa.bitwiseAnd(1 << 4))\n",
    "  return image.updateMask(cloud.Not())\n",
    "\n",
    "# Going to try removing the coverage overlap between LS5 and LS8 to try and fix some issues\n",
    "#LS5 = LS5.filterDate(start = '1984-01-01',opt_end = ee.Date('2013-04-11'))\n",
    "#LS8 = LS8.filterDate(start = ee.Date('2013-04-11'))\n",
    "\n",
    "# Lansat 5/7 & 8 differ in their band labeling, need to select the bands I'm going to use and rename them to\n",
    "# match each other before merging collections : bands I need [red,green,NIR,SWIR1,SWIR2]    \n",
    "LS8BandNames = ee.List(['B4','B3','B5','B6','B7','pixel_qa'])\n",
    "NewBandNames = ee.List(['B3','B2','B4','B5','B7','pixel_qa'])\n",
    "LS8 = LS8.select(LS8BandNames,NewBandNames)\n",
    "\n",
    "# Adding a function to calculate and add an NDVI band for a single image\n",
    "def addNDVI(image):\n",
    "  ndvi = image.normalizedDifference(['B4', 'B3']).rename('NDVI')\n",
    "  return image.addBands(ndvi)\n",
    "\n",
    "# Adding a function to calculate and add an NBR band for a single image.\n",
    "def addNBR(image):\n",
    "  nbr = image.normalizedDifference(['B4', 'B7']).rename('NBR')\n",
    "  return image.addBands(nbr)\n",
    "\n",
    "# Adding a function to calculate and add an MBI band for a single image.\n",
    "def addMBI(image):\n",
    "  MBI = image.expression(\n",
    "  \"MBI = ((b('B5') - b('B7') - b('B4')) / (b('B5') + b('B7') + b('B4'))) + 0.5\")\n",
    "  return image.addBands(MBI)\n",
    "\n",
    "# adding the cloud mask per generation\n",
    "LS5 = LS5.map(LScloudMask)\n",
    "LS7 = LS7.map(LScloudMask)\n",
    "LS8 = LS8.map(LScloudMask)\n",
    "\n",
    "# merging the landsat 5 and 7 collections\n",
    "LS_stack = LS5.merge(LS8)\n",
    "LS_stack = LS_stack.merge(LS7)\n",
    "\n",
    "# data reduction on the image stack\n",
    "LS_stack = LS_stack.filterBounds(loblolly)\n",
    "\n",
    "# Adding the indices to the filtered combined Landsat collection\n",
    "LS_stack_wVI = LS_stack.map(addNDVI)\n",
    "LS_stack_wVI = LS_stack_wVI.map(addNBR)\n",
    "LS_stack_wVI = LS_stack_wVI.map(addMBI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554452be",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdf60c8",
   "metadata": {},
   "source": [
    "## Stand Identification Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701417ad",
   "metadata": {},
   "source": [
    "### _Landcover/Landuse Mask_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1a7ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New NLCD/LCMS method\n",
    "# retrieve NLCD for each year\n",
    "NLCD_2001 = NLCD_col.filter(ee.Filter.eq('system:index', '2001')).first().select(\"landcover\")\n",
    "NLCD_2004 = NLCD_col.filter(ee.Filter.eq('system:index', '2004')).first().select(\"landcover\")\n",
    "NLCD_2006 = NLCD_col.filter(ee.Filter.eq('system:index', '2006')).first().select(\"landcover\")\n",
    "NLCD_2008 = NLCD_col.filter(ee.Filter.eq('system:index', '2008')).first().select(\"landcover\")\n",
    "NLCD_2011 = NLCD_col.filter(ee.Filter.eq('system:index', '2011')).first().select(\"landcover\")\n",
    "NLCD_2013 = NLCD_col.filter(ee.Filter.eq('system:index', '2013')).first().select(\"landcover\")\n",
    "NLCD_2016 = NLCD_col.filter(ee.Filter.eq('system:index', '2016')).first().select(\"landcover\")\n",
    "NLCD_2019 = NLCD_col.filter(ee.Filter.eq('system:index', '2019')).first().select(\"landcover\")\n",
    "\n",
    "# combine NLCD to image collection\n",
    "NLCDlandcover_col = ee.ImageCollection(ee.List([NLCD_2001,NLCD_2004,NLCD_2006,NLCD_2008,NLCD_2011,NLCD_2013,NLCD_2016,NLCD_2019]))\n",
    "\n",
    "# Function to remap NLCD classes of interest for conditional layer\n",
    "def remapNLCD(image):\n",
    "    image = ee.Image(image)\n",
    "    image = image.updateMask(ee.Image.constant(42).Or(ee.Image.constant(52)))\n",
    "    image = image.remap(ee.List([42,52]),ee.List([10,1]),defaultValue = None)\n",
    "    return image\n",
    "\n",
    "# Layer containing the summed values of pixels across the collection after remapping\n",
    "NLCDclassSum = NLCDlandcover_col.map(remapNLCD).reduce(ee.Reducer.sum())\n",
    "NLCDMask = NLCDclassSum.remap(ee.List([62,71,80]),ee.List([1,1,1]), defaultValue = None)\n",
    "\n",
    "# retrieve LCMS landuse classification\n",
    "LCMSlanduseCol = LCMS.select(\"Land_Use\")\n",
    "\n",
    "# A function to select only forest landuse class\n",
    "def remapLCMS(image):\n",
    "    image = ee.Image(image)\n",
    "    onlyForest = image.remap([3],[1], defaultValue = None)\n",
    "    return onlyForest\n",
    "\n",
    "LCMSlanduseSum = LCMSlanduseCol.map(remapLCMS).reduce(ee.Reducer.sum())\n",
    "\n",
    "# # combining the two layers into a landuse / landcover mask\n",
    "lulcMask = NLCDMask.updateMask(LCMSlanduseSum.gte(36))\n",
    "lulcMask = lulcMask.clip(loblolly) # clip mask to study boundaries for better loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69036e32",
   "metadata": {},
   "source": [
    "### _LCMS Fast change method_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1065ec27",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Using the LCMS Change metric to identify harvest areas in contrast to the max VI method\n",
    "# Filtering LCMS for the region and timeframe\n",
    "LCMSchange = LCMS.select('Change_Raw_Probability_Fast_Loss')\n",
    "\n",
    "def LCMSchangeSelection(image):\n",
    "    image = ee.Image(image)\n",
    "    minConfidence = 70\n",
    "    gtePercent = image.gte(ee.Image.constant(minConfidence))\n",
    "    gtePercent = gtePercent.updateMask(gtePercent.eq(1))\n",
    "    gtePercent = gtePercent.set({'year':image.date().get('year')})\n",
    "    outImage = gtePercent.updateMask(lulcMask).rename('remapped')\n",
    "    return outImage\n",
    "\n",
    "# applying the function to the LCMS\n",
    "FC_stack = LCMSchange.map(LCMSchangeSelection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f60b469",
   "metadata": {},
   "source": [
    "### _Connected Pixel (Min stand size) mask_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de135c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to apply a connected pixel mask to the input image\n",
    "def conectPixls(InImage,minArea,maxPixels):\n",
    "    pixelCount = InImage.connectedPixelCount(maxPixels,False)\n",
    "    minPixelCount = ee.Image(minArea).divide(ee.Image.pixelArea())\n",
    "    outImage = InImage.updateMask(pixelCount.gte(minPixelCount))\n",
    "    return outImage\n",
    "\n",
    "# a function to be mapped accross an image collection and annually apply the connected pixels mask, also creates an\n",
    "# additional band to store the year of disturbance for each pixel\n",
    "def annualConectPixls(image):\n",
    "    conectPixlsMasked = conectPixls(image,40000,1024) # minimum stand size of 4 ha (represented in m3), maximum of 92 ha (represented in pixel count) (tool limit)\n",
    "    imgYear = image.get('year')\n",
    "    imgYearBand = ee.Image.constant(imgYear).uint16().rename('ChangeY')\n",
    "    imgYearBand = imgYearBand.updateMask(conectPixlsMasked)\n",
    "    return conectPixlsMasked.addBands(imgYearBand)\n",
    "\n",
    "FC_final = FC_stack.map(annualConectPixls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c61b86c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the summary images\n",
    "FC_final_changeN = FC_final.select('remapped').reduce(ee.Reducer.sum())\n",
    "FC_final_firstYear = FC_final.select('ChangeY').reduce(ee.Reducer.min())\n",
    "FC_final_lastYear = FC_final.select('ChangeY').reduce(ee.Reducer.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649db560",
   "metadata": {},
   "source": [
    "### _Disturbance Year mask_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f683eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to help to keep detected disturbances within a given window of time\n",
    "# first detected disturbance\n",
    "FC_final_changeN = FC_final_changeN.updateMask(FC_final_firstYear.gte(startingD.get('year')) \\\n",
    "                                               .And(FC_final_firstYear.lte(endingD.get('year')))\n",
    "                                              )\n",
    "# last detected disturbance\n",
    "FC_final_changeN = FC_final_changeN.updateMask(FC_final_lastYear.gte(startingD.get('year')) \\\n",
    "                                               .And(FC_final_lastYear.lte(endingD.get('year')))\n",
    "                                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eb197f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final potential sample pixels\n",
    "# An image representing pixels that meet all selection criteria\n",
    "potentialSamples = ee.Image.toUint8(FC_final_changeN.updateMask(FC_final_changeN.eq(1))).rename('remapped_sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a73efb",
   "metadata": {},
   "source": [
    "### _Filter Selection to Only Include Homogenous, Non-Edge Groups of Pixels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85c74481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge avoidence\n",
    "PS_connectedPixelCount = potentialSamples.reduceNeighborhood(ee.Reducer.count(),\n",
    "                                                             ee.Kernel.circle(2, 'pixels', False, 1),\n",
    "                                                             'mask',\n",
    "                                                             True\n",
    "                                                            )\n",
    "potentialSamples2 = potentialSamples.updateMask(PS_connectedPixelCount.gte(13))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f4b76a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328cfac5",
   "metadata": {},
   "source": [
    "## Automatic Stand Selection Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bee631b",
   "metadata": {},
   "source": [
    "### _Creating Sampling Areas Using Ecoregions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc0cbc82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to convert the ecoregion code to an integer value\n",
    "def convertPropertyToBand(feat):\n",
    "    feat = ee.Feature(feat)\n",
    "    prop = feat.get('US_L3CODE')\n",
    "    propInt = ee.Number.parse(prop).toInt()\n",
    "    feat = feat.set({'numericL3ecocode':propInt})\n",
    "    return feat\n",
    "loblolly = loblolly.map(convertPropertyToBand)\n",
    "\n",
    "# Need to convert ecoregion feature collection and the property to integer in order for it to be used \n",
    "#     as the 'classBand' in the stratifiedSample fucntion\n",
    "ecoregionImage = ee.Image(loblolly.reduceToImage(['numericL3ecocode'],ee.Reducer.first()))\n",
    "ecoregionImage = ecoregionImage.cast({'first':'uint8'})\n",
    "ecoregionImage = ecoregionImage.clipToCollection(loblolly)\n",
    "\n",
    "# Adding ecoregion code as band to potential sample pixels\n",
    "potentialSamples2 = potentialSamples2.addBands(ecoregionImage.select('first').rename('numericL3ecocode'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefd8478",
   "metadata": {},
   "source": [
    "### _Creating Random Sample Points_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf09465a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Going to try just using the export table function to drive\n",
    "# samplePoints = potentialSamples2.stratifiedSample(numPoints = 50,\n",
    "#                                                  region = loblolly,\n",
    "#                                                  classBand = 'numericL3ecocode',\n",
    "#                                                  scale = 30,\n",
    "#                                                  seed = 5,\n",
    "#                                                  dropNulls = True,\n",
    "#                                                  geometries = True,\n",
    "#                                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1627a60b",
   "metadata": {},
   "source": [
    "### _Imports/Exports of Created Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90809049",
   "metadata": {},
   "outputs": [],
   "source": [
    "## today's date\n",
    "today = str(datetime.now()).split(\" \")[0]\n",
    "today = today.replace(\"-\",\"_\")\n",
    "today = \"_\"+today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c3a7c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the points created in the above cell to google drive (only way they will finish processing)\n",
    "# The export process will take about 15 minutes to complete\n",
    "#geemap.ee_export_vector_to_drive(samplePoints, 'EE_SamplePoints'+today, 'EarthEngine_Exports', file_format='shp', selectors=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "692c0ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Importing the points created in the above cell\n",
    "# samplePoints = ee.FeatureCollection('users/dputnam21/stratifiedSamplePoints_03022022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9965f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing points created in arcpro\n",
    "samplePoints = ee.FeatureCollection('users/dputnam21/samplePoints_05252022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca6dfc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exporting to google drive the NLCD/LCMS masked LCMS fast change summary layer\n",
    "# geemap.ee_export_image_to_drive(potentialSamples2, description='PotentialSamples'+today,region = loblolly.geometry(), folder='EarthEngine_Exports', scale=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b09d5c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65253083",
   "metadata": {},
   "source": [
    "### _Creating Point Buffers for Sampling_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7b6b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduction in the number of samplepoints for methodology testing\n",
    "numSamples = 500\n",
    "samplePoints2 = samplePoints.filter(ee.Filter.lt('UniqueID',numSamples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "232ccf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to be mapped over point feature collection to create buffers\n",
    "def makeBuffers(feat):\n",
    "    inFeat = ee.Feature(feat)\n",
    "    buff = inFeat.buffer(distance = 60) # 60 meter buffer = 2 pixel radius to align with circlular kernel mask\n",
    "    return buff\n",
    "\n",
    "sampleCircles = samplePoints2.map(makeBuffers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95de9c96",
   "metadata": {},
   "source": [
    "### Displaying images on the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c9fb17f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c3817cc8754ed1b8fe87a4c1714e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[36.8497, -77.2013], controls=(WidgetControl(options=['position', 'transparent_bg'], widget=HBox(ch…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LCMS landcover palette\n",
    "LCMSlcPalette = ['efff6b','ff2ff8','1b9d0c','97ffff','a1a1a1','c2b34a','1B1716']\n",
    "\n",
    "Map = geemap.Map(basemap=\"SATELLITE\")\n",
    "#Map.centerObject(loblolly,7)\n",
    "Map.centerObject(ee.Feature(ee.Geometry.Point([-77.2013,36.8497, ])),13)\n",
    "\n",
    "# This is the bottom of the layer order\n",
    "\n",
    "# Map.addLayer(protectedAreas)\n",
    "# Map.addLayer(GAP)\n",
    "Map.addLayer(ecoregionImage.select('first'), vis_params = {'palette': LCMSlcPalette, 'min': 45, 'max':75}, name = 'Ecoregion Code Image',shown = False)\n",
    "Map.addLayer(NLCDMask, vis_params = {'palette': ['2ca25f'],'min':1,'max':1}, name = 'NLCD Mask', shown = False)\n",
    "Map.addLayer(lulcMask, vis_params = {'palette': ['99d8c9'],'min':1,'max':1}, name = 'LCMS + NLCD landcover mask', shown = False)\n",
    "Map.addLayer(FC_stack, vis_params = {'palette': ['e34a33'],'min':0,'max':1}, name = 'FC (>70%) raw', shown = False)\n",
    "Map.addLayer(FC_final.select('remapped'), vis_params = {'palette': ['3182bd'],'min':0,'max':1}, name = 'FC (>70%) Min Area', shown = False)\n",
    "Map.addLayer(FC_final_changeN,{'palette':['fee0d2','fc9272','de2d26'],'min':1,'max':5},'LCMS Fast Change Count',True)\n",
    "Map.addLayer(FC_final_firstYear,{'palette':['edf8b1','7fcdbb','2c7fb8'],'min' : 1995, 'max' : 2010},'LCMS Fast Change Year',False)\n",
    "Map.addLayer(potentialSamples2.select('remapped_sum'),{'palette':['fee0d2','fc9272','de2d26'],'min':0,'max':1}, name = 'Potential Samples 2', shown = True)\n",
    "Map.addLayer(samplePoints,{'color':'blue'}, name = 'Stratified Random Samples',shown = True)\n",
    "# Map.addLayer(NLCDclassSum,vis_params = {'palette':['edf8e9','bae4b3','74c476','31a354','006d2c'],'min':0,'max':80})\n",
    "# Map.addLayer(ee.Feature(ee.Geometry.Point([-82.1455878,33.6141944])),{'color':'blue'})\n",
    "\n",
    "# This is the top of the layer order\n",
    "\n",
    "# Adding a legend for exporting images of layers\n",
    "legend_keys = ['Final Potential Sample Pixels', 'Edge Pixels Removed']\n",
    "# colorS can be defined using either hex code or RGB (0-255, 0-255, 0-255)\n",
    "legend_colors = ['de2d26','fee0d2']\n",
    "\n",
    "Map.add_legend(\n",
    "    legend_keys=legend_keys, legend_colors=legend_colors, position='bottomright'\n",
    ")\n",
    "\n",
    "Map.addLayerControl()\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37454912",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6664e8ed",
   "metadata": {},
   "source": [
    "## Automatic Stand Selection Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6877e2f6",
   "metadata": {},
   "source": [
    "### _Export of stand attributes & Environmental Variables_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba6a2958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of environmental data\n",
    "elevation_image = ee.Image(\"USGS/3DEP/10m\").select('elevation')\n",
    "soilTexture_image = ee.Image(\"OpenLandMap/SOL/SOL_TEXTURE-CLASS_USDA-TT_M/v02\")\n",
    "climate_col = ee.ImageCollection(\"NASA/NEX-DCP30_ENSEMBLE_STATS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cffe44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f7509134b445c5a5eca17af57d732d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[33.95843232835059, -85.41690891233773], controls=(WidgetControl(options=['position', 'transparent_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Map2 = geemap.Map(basemap=\"SATELLITE\")\n",
    "Map2.centerObject(loblolly,5)\n",
    "\n",
    "Map2.addLayer(climate_col.first().select('pr_mean'),vis_params = {'palette': ['white','blue'],'min':0,'max':0.00009,'opacity':0.75}, name = 'Precipitation')\n",
    "Map2.addLayer(loblolly, name = 'Ecoregion Polygons',shown = True)\n",
    "\n",
    "\n",
    "Map2.addLayerControl()\n",
    "Map2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f6d0f2",
   "metadata": {},
   "source": [
    "### _Ecoregion, Location, Topography, Soil Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d98e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating topography variables from the elevation layer\n",
    "topography = ee.Terrain.products(elevation_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf95254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting topography and soil texture variables\n",
    "\n",
    "# Mean elevation\n",
    "stands_elevation_FC = topography.select('elevation').reduceRegions(collection = sampleCircles,\n",
    "                                                                   reducer = ee.Reducer.mean(),\n",
    "                                                                   scale = 30\n",
    "                                                                  )\n",
    "elevationList = stands_elevation_FC.aggregate_array('mean').getInfo()\n",
    "\n",
    "# Mean slope\n",
    "stands_slope_FC = topography.select('slope').reduceRegions(collection = sampleCircles,\n",
    "                                                           reducer = ee.Reducer.mean(),\n",
    "                                                           scale = 30\n",
    "                                                          )\n",
    "slopeList = stands_slope_FC.aggregate_array('mean').getInfo()\n",
    "\n",
    "# Mean Aspect\n",
    "stands_slope_FC = topography.select('aspect').reduceRegions(collection = sampleCircles,\n",
    "                                                           reducer = ee.Reducer.mean(),\n",
    "                                                           scale = 30\n",
    "                                                          )\n",
    "aspectList = stands_slope_FC.aggregate_array('mean').getInfo()\n",
    "\n",
    "# Mode Soil Texture @ 0cm from surface\n",
    "stands_soil0_FC = soilTexture_image.select('b0').reduceRegions(collection = sampleCircles,\n",
    "                                                               reducer = ee.Reducer.mode(),\n",
    "                                                               scale = 250\n",
    "                                                              )\n",
    "soilList0 = stands_soil_FC.aggregate_array('mode').getInfo()\n",
    "\n",
    "# Mode Soil Texture @ 30cm from surface\n",
    "stands_soil30_FC = soilTexture_image.select('b30').reduceRegions(collection = sampleCircles,\n",
    "                                                               reducer = ee.Reducer.mode(),\n",
    "                                                               scale = 250\n",
    "                                                              )\n",
    "soilList30 = stands_soil_FC.aggregate_array('mode').getInfo()\n",
    "\n",
    "# Mode Soil Texture @ 100cm from surface\n",
    "stands_soil100_FC = soilTexture_image.select('b100').reduceRegions(collection = sampleCircles,\n",
    "                                                               reducer = ee.Reducer.mode(),\n",
    "                                                               scale = 250\n",
    "                                                              )\n",
    "soilList100 = stands_soil_FC.aggregate_array('mode').getInfo()\n",
    "\n",
    "# the points are ordered weird in earth engine, this will need to be used to rearrange them\n",
    "IDarray = sampleCircles.aggregate_array('UniqueID').getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81189ef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extracting the locational and ecotonal information for each stand, assigning all exported topography and soil attributes\n",
    "#      to a dataframe to be exported\n",
    "\n",
    "# Timer\n",
    "start = datetime.now() # figuring out how long this takes to run\n",
    "print(\"Extraction initiated :\",start)\n",
    "\n",
    "# list of unique ID's for sample points\n",
    "stand_nums = range(0,numSamples)\n",
    "\n",
    "# Creating a dataframe for the stand attributes\n",
    "standAttributes = pd.DataFrame(index = stand_nums, columns = ['lat','long','ecoR_code','elevation','slope','aspect','soilCode0','soilCode30','soilCode100'])\n",
    "\n",
    "# Getting the lat and long values from the feature collection\n",
    "for ID in stand_nums :\n",
    "    featureDict = samplePoints2.filter(ee.Filter.eq('UniqueID',ID)).first().getInfo()\n",
    "    standAttributes.iloc[ID,0] = featureDict['geometry']['coordinates'][1]\n",
    "    standAttributes.iloc[ID,1] = featureDict['geometry']['coordinates'][0]\n",
    "    standAttributes.iloc[ID,2] = featureDict['properties']['Classified']\n",
    "    arrayIndex = IDarray.index(ID)\n",
    "    standAttributes.iloc[ID,3] = elevationList[arrayIndex]\n",
    "    standAttributes.iloc[ID,4] = slopeList[arrayIndex]\n",
    "    standAttributes.iloc[ID,5] = aspectList[arrayIndex]\n",
    "    standAttributes.iloc[ID,6] = soilList0[arrayIndex]\n",
    "    standAttributes.iloc[ID,7] = soilList30[arrayIndex]\n",
    "    standAttributes.iloc[ID,8] = soilList100[arrayIndex]\n",
    "    \n",
    "end = datetime.now() # effectively ending the timer\n",
    "duration = end - start\n",
    "# nicely printing the ellapsed time\n",
    "timeList = str(duration).split(':')\n",
    "print('The time elapsed during this execution of this operation was :','\\n',\n",
    "     timeList[0],'Hour(s)','\\n',\n",
    "     timeList[1],'Minute(s)','\\n',\n",
    "      'and',round(float(timeList[2]),ndigits = 0),'Seconds'\n",
    "     )\n",
    "    \n",
    "standAttributes.to_csv(path_or_buf='C:/R_workspace/standAttributes'+today+'.csv', sep=',', na_rep='', float_format=None, header=True, index=True, mode='w')\n",
    "    \n",
    "standAttributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c0807c",
   "metadata": {},
   "source": [
    "### _Climate Data Extraction_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d749dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to provide null value of 0.0 whenever there is missing data\n",
    "def fillNA(point): \n",
    "    infeat = ee.Feature(point)\n",
    "    val = infeat.get('first')\n",
    "    newVal = ee.List([val, 0.0]).reduce(ee.Reducer.firstNonNull())\n",
    "    outfeat = infeat.set({'first':newVal})\n",
    "    return outfeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a2826633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction initiated : 2022-06-08 11:47:09.229089\n",
      "rcp26\n",
      "1984->-1985->-1986->-1987->-1988->-1989->-1990->-1991->-1992->-1993->-1994->-1995->-1996->-1997->-1998->-1999->-2000->-2001->-2002->-2003->-2004->-2005->-2006->-2007->-2008->-2009->-2010->-2011->-2012->-2013->-2014->-2015->-2016->-2017->-2018->-2019->-2020->-2021->-\n",
      "\n",
      "rcp45\n",
      "1984->-1985->-1986->-1987->-1988->-1989->-1990->-1991->-1992->-1993->-1994->-1995->-1996->-1997->-1998->-1999->-2000->-2001->-2002->-2003->-2004->-2005->-2006->-2007->-2008->-2009->-2010->-2011->-2012->-2013->-2014->-2015->-2016->-2017->-2018->-2019->-2020->-2021->-\n",
      "\n",
      "rcp60\n",
      "1984->-1985->-1986->-1987->-1988->-1989->-1990->-1991->-1992->-1993->-1994->-1995->-1996->-1997->-1998->-1999->-2000->-2001->-2002->-2003->-2004->-2005->-2006->-2007->-2008->-2009->-2010->-2011->-2012->-2013->-2014->-2015->-2016->-2017->-2018->-2019->-2020->-2021->-\n",
      "\n",
      "rcp85\n",
      "1984->-1985->-1986->-1987->-1988->-1989->-1990->-1991->-1992->-1993->-1994->-1995->-1996->-1997->-1998->-1999->-2000->-2001->-2002->-2003->-2004->-2005->-2006->-2007->-2008->-2009->-2010->-2011->-2012->-2013->-2014->-2015->-2016->-2017->-2018->-2019->-2020->-2021->-\n",
      "\n",
      "The time elapsed during this execution of this operation was : \n",
      " 0 Hour(s) \n",
      " 25 Minute(s) \n",
      " and 18.0 Seconds\n"
     ]
    }
   ],
   "source": [
    "# Extracting time series for each climate scenario for each stand\n",
    "start = datetime.now() # figuring out how long this takes to run\n",
    "print(\"Extraction initiated :\",start)\n",
    "\n",
    "singleYlists = []\n",
    "\n",
    "scenarioValueLists = []\n",
    "\n",
    "scenarioList = ['rcp26','rcp45','rcp60','rcp85']\n",
    "\n",
    "for i in range(len(scenarioList)) :\n",
    "    print(scenarioList[i])\n",
    "    climateScenario_col = climate_col.filter(ee.Filter.inList('scenario',[scenarioList[i],'historical']))\n",
    "    \n",
    "    for year in range(1984,2022): # 2022 not inclusive, collection 1 ends at end of 2021\n",
    "        print(year,end = '->-')\n",
    "        filteredColl = climateScenario_col.filter(ee.Filter.calendarRange(year, year, 'year'))\n",
    "        threeBandImage = filteredColl.reduce(ee.Reducer.mean()) # mean annual value for each variable\n",
    "        proj = potentialSamples2.select('remapped_sum').projection()\n",
    "        if len(threeBandImage.bandNames().getInfo()) < 3 :\n",
    "            singleYlists.append([None]*len(stand_nums))\n",
    "        else :\n",
    "        \n",
    "            # Extracting precipitation values\n",
    "            pointPrecipValCol = threeBandImage.select('pr_mean_mean').reduceRegions(collection = sampleCircles,\n",
    "                                                        reducer = ee.Reducer.first(), \n",
    "                                                        crs = proj,\n",
    "                                                        scale = 30 # Same scale as projection and landsat data\n",
    "                                                        )\n",
    "\n",
    "            pointPrecipValCol = pointPrecipValCol.set({'year':year})\n",
    "            pointPrecipValCol = pointPrecipValCol.map(fillNA) \n",
    "            PrecipVals = pointPrecipValCol.aggregate_array('first').getInfo()\n",
    "\n",
    "            # Extracting min temp values\n",
    "            pointMinTempValCol = threeBandImage.select('tasmin_mean_mean').reduceRegions(collection = sampleCircles,\n",
    "                                                        reducer = ee.Reducer.first(), \n",
    "                                                        crs = proj,\n",
    "                                                        scale = 30 # Same scale as projection and landsat data\n",
    "                                                        )\n",
    "\n",
    "            pointMinTempValCol = pointMinTempValCol.set({'year':year})\n",
    "            pointMinTempValCol = pointMinTempValCol.map(fillNA) \n",
    "            minTempVals = pointMinTempValCol.aggregate_array('first').getInfo()\n",
    "\n",
    "            # Extracting max temp values\n",
    "            pointMaxTempValCol = threeBandImage.select('tasmax_mean_mean').reduceRegions(collection = sampleCircles,\n",
    "                                                        reducer = ee.Reducer.first(), \n",
    "                                                        crs = proj,\n",
    "                                                        scale = 30 # Same scale as projection and landsat data\n",
    "                                                        )\n",
    "\n",
    "            pointMaxTempValCol = pointMaxTempValCol.set({'year':year})\n",
    "            pointMaxTempValCol = pointMaxTempValCol.map(fillNA) \n",
    "            maxTempVals = pointMaxTempValCol.aggregate_array('first').getInfo()\n",
    "\n",
    "            # Creating a list to store all three variable lists for a given year\n",
    "\n",
    "\n",
    "            singleYlists.append([PrecipVals,minTempVals,maxTempVals])\n",
    "            \n",
    "    scenarioValueLists.append(singleYlists)\n",
    "    singleYlists = []\n",
    "    print('\\n')\n",
    "\n",
    "#### ending the timer ####\n",
    "end = datetime.now() \n",
    "duration = end - start\n",
    "# nicely printing the ellapsed time\n",
    "timeList = str(duration).split(':')\n",
    "print('The time elapsed during this execution of this operation was :','\\n',\n",
    "     timeList[0],'Hour(s)','\\n',\n",
    "     timeList[1],'Minute(s)','\\n',\n",
    "      'and',round(float(timeList[2]),ndigits = 0),'Seconds'\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e137c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling the values out of the mess created above\n",
    "yearList = list(range(1984,2022))\n",
    "\n",
    "# Have to integrate the weirdly organized uniqueID's\n",
    "standIDs = pointPrecipValCol.aggregate_array('UniqueID').getInfo()\n",
    "\n",
    "# Creating a seperate dataframe for each climate variable and each climate scenario\n",
    "rcp26_precipDF = pd.DataFrame(index = standIDs, columns = yearList)\n",
    "rcp26_minTempDF = pd.DataFrame(index = standIDs, columns = yearList)\n",
    "rcp26_maxTempDF = pd.DataFrame(index = standIDs, columns = yearList)\n",
    "\n",
    "rcp45_precipDF = pd.DataFrame(index = standIDs, columns = yearList)\n",
    "rcp45_minTempDF = pd.DataFrame(index = standIDs, columns = yearList)\n",
    "rcp45_maxTempDF = pd.DataFrame(index = standIDs, columns = yearList)\n",
    "\n",
    "rcp60_precipDF = pd.DataFrame(index = standIDs, columns = yearList)\n",
    "rcp60_minTempDF = pd.DataFrame(index = standIDs, columns = yearList)\n",
    "rcp60_maxTempDF = pd.DataFrame(index = standIDs, columns = yearList)\n",
    "\n",
    "rcp85_precipDF = pd.DataFrame(index = standIDs, columns = yearList)\n",
    "rcp85_minTempDF = pd.DataFrame(index = standIDs, columns = yearList)\n",
    "rcp85_maxTempDF = pd.DataFrame(index = standIDs, columns = yearList)\n",
    "\n",
    "# dealing with the mess, sorting these values out into their respective dataframes \n",
    "for scenario in scenarioList :\n",
    "    scenarioIndex = scenarioList.index(scenario)\n",
    "    singleScenario_allYears = scenarioValueLists[scenarioIndex]\n",
    "    for year in yearList :\n",
    "        yearIndex = yearList.index(year)\n",
    "        singleYear_allVariables = singleScenario_allYears[yearIndex]\n",
    "        locals()[scenario+'_precipDF'].iloc[:,yearIndex] = singleYear_allVariables[0]\n",
    "        locals()[scenario+'_minTempDF'].iloc[:,yearIndex] = singleYear_allVariables[1]\n",
    "        locals()[scenario+'_maxTempDF'].iloc[:,yearIndex] = singleYear_allVariables[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "46df972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exporting the dataframes for each scenario and value\n",
    "for scenario in scenarioList :\n",
    "    locals()[scenario+'_precipDF'].to_csv(path_or_buf=\"C:/R_workspace/\"+\"precipDF_\"+scenario+today+\".csv\", sep=',', na_rep='', float_format=None, header=True, index=True, mode='w')\n",
    "    locals()[scenario+'_minTempDF'].to_csv(path_or_buf=\"C:/R_workspace/\"+\"minTempDF_\"+scenario+today+\".csv\", sep=',', na_rep='', float_format=None, header=True, index=True, mode='w')\n",
    "    locals()[scenario+'_maxTempDF'].to_csv(path_or_buf=\"C:/R_workspace/\"+\"maxTempDF_\"+scenario+today+\".csv\", sep=',', na_rep='', float_format=None, header=True, index=True, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e649041",
   "metadata": {},
   "source": [
    "### _New, newer compositing procedure_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a5bc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter analysis parameters\n",
    "compositeMonthStart = 2 \n",
    "compositeMonthEnd = 3 #inclusive\n",
    "outputIndex = 'NBR'\n",
    "compositeStat = 'median'\n",
    "\n",
    "# prep for function\n",
    "chart_VI = LS_stack_wVI.filter(ee.Filter.calendarRange(compositeMonthStart,compositeMonthEnd,'month'));\n",
    "\n",
    "years = ee.List.sequence(1984, 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81daf14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interting a single constant image into the landsat stack to avoid missing data in 1990\n",
    "zeroImage = ee.Image.constant(0.0).clipToBoundsAndScale(loblolly.geometry(),scale = 30).toFloat()\n",
    "zeroImage = zeroImage.rename(['NBR'])\n",
    "zeroImage = zeroImage.set('system:time_start', ee.Date.fromYMD(1990,2,1).millis())\n",
    "chart_VI = chart_VI.merge(zeroImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c7f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out a new method because the old method is now too slow\n",
    "start = datetime.now() # figuring out how long this takes to run\n",
    "print(\"Extraction initiated :\",start)\n",
    "\n",
    "listOfLists = []\n",
    "\n",
    "for year in range(1984,2022): # 2022 not inclusive, collection 1 ends at end of 2021\n",
    "    print(year)\n",
    "    if year != 1990 :\n",
    "        filteredColl = chart_VI.filter(ee.Filter.calendarRange(year, year, 'year'))\n",
    "        singleImage = filteredColl.select(outputIndex).reduce(ee.Reducer.median()) ## CHANGE COMPOSITE STAT HERE ###\n",
    "        proj = potentialSamples2.select('remapped_sum').projection()\n",
    "        pointValCol = singleImage.reduceRegions(collection = sampleCircles,\n",
    "                                                reducer = ee.Reducer.mean(), \n",
    "                                                crs = proj,\n",
    "                                                scale = 30\n",
    "                                                )\n",
    "        pointValCol = pointValCol.set({'year':year})\n",
    "        pointValCol = pointValCol.map(fillNA) \n",
    "        NBRvalues = pointValCol.aggregate_array('mean').getInfo()\n",
    "        listOfLists.append(NBRvalues)\n",
    "    else :\n",
    "        NBRvalues = [0.0]*len(stand_nums)\n",
    "        listOfLists.append(NBRvalues)\n",
    "\n",
    "#### ending the timer ####\n",
    "end = datetime.now() \n",
    "duration = end - start\n",
    "# nicely printing the ellapsed time\n",
    "timeList = str(duration).split(':')\n",
    "print('The time elapsed during this execution of this operation was :','\\n',\n",
    "     timeList[0],'Hour(s)','\\n',\n",
    "     timeList[1],'Minute(s)','\\n',\n",
    "      'and',round(float(timeList[2]),ndigits = 0),'Seconds'\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6b71cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### this function was in the middle of the above script (between the two pointValCol assignments)\n",
    "# ###       I don't think I need it anymore (the function is defined in an above cell) but I'm not 100%\n",
    "\n",
    "#         def fillNA(point): \n",
    "#             infeat = ee.Feature(point)\n",
    "#             NBRval = infeat.get('mean')\n",
    "#             newVal = ee.List([NBRval, 0.0]).reduce(ee.Reducer.firstNonNull())\n",
    "#             outfeat = infeat.set({'mean':newVal})\n",
    "#             return outfeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a0f3d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "listIndex = 0\n",
    "for year in range(1984,2022):\n",
    "    print(year,'\\n')\n",
    "    print('Size', len(listOfLists[listIndex]),'\\n')\n",
    "    print(listOfLists[listIndex],'\\n')\n",
    "    listIndex += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74c7e13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# client side replacement of null value (0.0) with null object\n",
    "NDVIvals = listOfLists\n",
    "for i in range(0,len(NDVIvals)):\n",
    "    for i2 in range(len(NDVIvals[i])) :\n",
    "        if NDVIvals[i][i2] == 0 :\n",
    "            NDVIvals[i][i2] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f364f87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating the dataframe\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "NDVItsDF = pd.DataFrame(index = IDarray, columns = imageYears)\n",
    "for c in range(0,len(NDVIvals)):\n",
    "    NDVItsDF.iloc[:,c] = NDVIvals[c]\n",
    "        \n",
    "for col in NDVItsDF:\n",
    "    NDVItsDF[col] = pd.to_numeric(NDVItsDF[col], errors='coerce')\n",
    "NDVItsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeee48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exporting the dataframe before interpolating missing values\n",
    "NDVItsDF.to_csv(path_or_buf=\"C:/R_workspace/timeSeriesDF500\"+today+\"_wNA.csv\", sep=',', na_rep='', float_format=None, header=True, index=True, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54df0acc",
   "metadata": {},
   "source": [
    "### Creating time series plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072d15a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy as sp\n",
    "import scipy.signal as scisig\n",
    "\n",
    "## interpolation of null values\n",
    "plotDF = NDVItsDF.interpolate(axis = 'columns',method = 'akima')\n",
    "\n",
    "# setting up for time-series plots\n",
    "fig, axs = plt.subplots(7, 3, sharex=False, sharey=True, figsize = (16,25))\n",
    "\n",
    "for i, ax in enumerate(fig.axes):\n",
    "    ax.plot(plotDF.iloc[i,].transpose(),label = \"Interpolated\")\n",
    "    ax.plot(NDVItsDF.iloc[i,].transpose(),label = \"Original\")\n",
    "    ax.scatter(imageYears,NDVItsDF.iloc[i,], color = 'orange',s = 20)\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_title(\"Stand\"+' '+str(IDarray[i])+' '+\"Time-Series\")\n",
    "    \n",
    "fig.subplots_adjust(left=0.1, bottom=0.1, right=None, top=None, wspace=0.3, hspace=0.5)\n",
    "fig.text(0.5, 0.04, 'Year', ha='center', va='center')\n",
    "fig.text(0.06, 0.5, (outputIndex+' '+'value'), ha='center', va='center', rotation='vertical')\n",
    "\n",
    "fig.savefig(\"C:/R_workspace/timeSeries_interpolation.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883b3f46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotDF\n",
    "\n",
    "plotDF.to_csv(path_or_buf=\"C:/R_workspace/timeSeriesDF500\"+today+\".csv\", sep=',', na_rep='', float_format=None, header=True, index=True, mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
